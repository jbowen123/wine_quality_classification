---
title: "Wine Classification Project"
author: "Judy Bowen"
date: "2/25/2022"
output:
  pdf_document: 
    latex_engine: xelatex
---

```{r, setup, include=FALSE, eval=TRUE}

knitr::opts_chunk$set(fig.width=8)

```

```{r, message=FALSE, warning=FALSE}

```

# **Executive Summary** {-}

The purpose of this project is to use R to build a recommendation system that helps people find a good wine, based upon the chemical composition of the wine and wine ratings from third party wine tasters.  Modeling wine preferences may be useful not only for marketing purposes but also to improve wine production or support the oenologist wine tasting evaluations.   The dataset is obtained as a download from the Kaggle website.  As described on Kaggle.com, the dataset is related to red and white variants of the Portuguese "Vinho Verde" wine as compiled in Cortez et al, 2009.^[P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties.  In Decision Support Systems, Elsevier, 47(4):547-553, 2009.] This dataset is also available from the UCI machine learning repository^[https://archive.ics.uci.edu/ml/datasets/wine+quality].  Due to privacy and logistic issues, only physiochemical (inputs) and sensory (the output of ‘quality’, as ranked from 1 to 10) variables are available (e.g. there is no data about grape types, wine brand, or wine selling price). 

Input variables are (based on physicochemical tests): fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol content.  The output quality variable is based on sensory data, as a score between 0 and 10.  In this paper, the quality variable is further defined by a grouping into two broad categories:  quality measures of 7 and above are defined as group 1 and measures below 7 group 0.  The purpose of the model is to identify ‘good’ wines in group 1.  

The data set is first described by correlation estimates and then by eigenvectors of principal component analysis, to discover relationships between the chemical composition input variables and the categorical quality output variable.  Following the exploratory data analysis, several models were selected to find the best way to predict a ‘good’ wine: a random forest decision tree, a logistic regression model and a boosted gradient model are designed and tested.  

The results of the data analysis showed that of all models tested here for identifying ‘good’ wines, none could satisfactorily identify all of the good wines in the test data set.  The options for identifying good wine were either of two:  select the models which identify a small portion of good wines but which did not also incorrectly identify a larger number of poor wines as good wines or; identify most of the good wines but also falsely identify a large number of poor wines as good wines.  Improving the models by moving the threshold of quality in the models has been identified as an unsatisfactory fix to resolve the problem.     

```{r start, include=FALSE, echo=FALSE}
### Create wine data set,  
##########################################################

# Note: this process could take a couple of minutes
# Note: this process could take a couple of minutes
if(!require(Rtools)) install.packages("Rtools", repos = "http://cran.us.r-project.org")

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(bookdown)) install.packages("bookdown", repos = "http://cran.us.r-project.org")
library(bookdown)
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(gglot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(viridis)) install.packages("viridis", repos = "http://cran.us.r-project.org")

if(!require(tidyselect)) install.packages("tidyselect", repos = "http://cran.us.r-project.org")

###load the applications required for the cross validated logistic regression, the randomForest model and the gbs
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
library(randomForest)
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
library(caret)
if(!require(caTools)) install.packages("caTools", repos = "http://cran.us.r-project.org")
library(caTools)  # For Logistic regression
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org")
library(pROC)  # For ROC curve to evaluate model
if(!require(mltools)) install.packages("mltools", repos = "http://cran.us.r-project.org")
library(mltools)  # For ROC curve to evaluate model
#if(!require(predictABEL)) install.packages("predictABEL", repos = "https://rdocumentation.org/packages/PredictABEL/versions/1.2-4")

#library(predictABEL)  # For ROC curve to evaluate model
if(!require(plotROC)) install.packages("plotROC", repos = "http://cran.us.r-project.org")
library(plotROC)  # For ROC curve to evaluate model

if(!require(ROCR)) install.packages("ROCR", repos = "http://cran.us.r-project.org")
library(ROCR)  # For ROC curve to evaluate model

if(!require(MLeval)) install.packages("MLeval", repos = "http://cran.us.r-project.org")
library(MLeval) # for functional programming (map)
if(!require(MLmetrics)) install.packages("MLmetrics", repos = "http://cran.us.r-project.org")
library(MLmetrics) # for functional programming (map)
######tests for multicollinearity######
if(!require(multiColl)) install.packages("multiColl", repos = "http://cran.us.r-project.org")
library(multiColl) # for tests of multicollinearity

if(!require(purrr)) install.packages("purrr", repos = "http://cran.us.r-project.org")
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org")

library(purrr) # for functional programming (map)

if(!require(robustbase)) install.packages("robustbase", repos = "http://cran.us.r-project.org")

if(!require(matrixStats)) install.packages("matrixStats", repos = "http://cran.us.r-project.org")

if(!require(rstatix)) install.packages("rstatix", repos = "http://cran.us.r-project.org")
library(rstatix)
if(!require(RColorBrewer)) install.packages("RColorBrewer", repos = "http://cran.us.r-project.org")
library(RColorBrewer)
if(!require(car)) install.packages("car", repos = "http://cran.us.r-project.org")
library(car)

##install the factoMineR and factoextra packages
if(!require(cluster)) install.packages("cluster", repos = "http://cran.us.r-project.org")
if(!require(FactoMineR)) install.packages("FactoMineR", repos = "http://cran.us.r-project.org")
if(!require(factoextra)) install.packages("factoextra", repos = "http://cran.us.r-project.org")
library(FactoMineR)
library(factoextra)
library(cluster)

if(!require(ellipse)) install.packages("ellipse", repos = "http://cran.us.r-project.org")
library(ellipse)
if(!require(corrgram)) install.packages("corrgram", repos = "http://cran.us.r-project.org")
library(corrgram)

library(matrixStats)
library(robustbase)
if(!require(latexpdf)) install.packages("latexpdf", repos = "http://cran.us.r-project.org")

library(latexpdf)
#tinytex::install_tinytex()

if(!require(tinytex)) install.packages("tinytex", repos = "http://cran.us.r-project.org")

library(tinytex)
library(tidyverse)
library(caret)
library(data.table)
library(dplyr)
library(vctrs)
library(forcats)
library(ggplot2)
#install.packages("viridis")
#library(viridis)
library(tidyselect)
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")

library(corrplot)
if(!require(GGally)) install.packages("GGally", repos = "http://cran.us.r-project.org")
library(GGally)
if(!require(ggpubr)) install.packages("ggpubr", repos = "http://cran.us.r-project.org")

library(ggpubr)
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org")
library(e1071)
if(!require(rmarkdown)) install.packages("rmarkdown", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
library(rmarkdown)
library(knitr)
if(!require(utils)) install.packages("utils", repos = "http://cran.us.r-project.org")
library(utils)
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
library(kableExtra)
if(!require(rio)) install.packages("rio", repos = "http://cran.us.r-project.org")
library(rio)


```


```{r portons, echo=FALSE, include=FALSE}

##set the working directory
winedataimport <- import("https://raw.githubusercontent.com/jbowen123/wine_quality_classification/main/winequality.csv")

winequality <- winedataimport
head(winequality)
table_1 <- summary(winequality)
#table_1
str(winequality)
##look at the dataset
table(winequality$quality)
get_counts <- winequality %>% mutate(quality=as.factor(quality)) %>% group_by(quality) %>% summarise(n=n())
data.frame(get_counts)
str(get_counts)

##remove the duplicate records
winequality_nd <- winequality[!duplicated(winequality),]
table_2 <- summary(winequality_nd)
str(table_2)
winequality_nd_count <- winequality_nd %>% group_by(quality) %>% summarise(count=n())
winequality_nd_count
table(winequality_nd$quality)
duplicates <- winequality[duplicated(winequality),]
summary(duplicates)
str(duplicates)
duplicates1 <- duplicates %>%mutate(quality=as.factor(quality)) %>% group_by(quality) %>% summarise(duplicate_count=n())
str(duplicates1)
##add two rows to duplicates1 for zero 3 and zero 4
rows <- rbind(c(3,0), c(4,0)) 
colnames(rows) <- c("quality", "duplicate_count")
rows1 <- as.tibble(rows) %>% mutate(quality=as.factor(quality), duplicate_count=as.numeric(duplicate_count))
str(rows1)
rows2 <- data.frame(rows1)
str(rows2)
duplicates3 <- data.frame(duplicates1) #%>% mutate(quality=as.factor(quality))
str(duplicates3)
colnames(get_counts) <- c("quality", "original_count")

all_duplicate_counts <- rbind(rows2,duplicates3) 
str(all_duplicate_counts) 


##now left join the get_counts tibble (convert to a data.frame first) with all_duplicate_counts so we get a portion of totals
more_duplicate <- left_join(all_duplicate_counts, as.data.frame(get_counts), by="quality") %>% mutate(original_count=as.numeric(original_count))
str(more_duplicate)

duplicate.portions <- more_duplicate %>% mutate(Total_duplicates=sum(duplicate_count), Original_Total=sum(original_count), Fraction_of_original_count= (duplicate_count/original_count))
duplicate.portions

str(duplicate.portions)
duplicate_portions <- format(duplicate.portions[, c(1,2,3,6)],digits=1)

```




## **Dataset Description and Analysis**
The dataset was downloaded from the Kaggle website^[https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009] .  There are no empty data cells.  However, there are duplicate entries in the database.  The original dataset contained 12 columns and 1599 rows.  After running the R duplicated function to remove duplicate records, the database contained 12 columns and 1359 rows. 

There is no evidence that the duplicate entries were more frequently associated with a particular quality variable, when taking the fraction of duplicate records over total record count by each quality group:



```{r duplicate_portions, echo=FALSE}

kable(duplicate_portions, caption="Duplicate record portions removed from each wine quality class", align = "lrrr", booktabs=TRUE, "latex")
```

The following paragraphs describe the dataset, with duplicate records removed.
A histogram of the wine quality variable indicates that the distribution is expected to be normal with most wines classified in the middle qualities of five and six, with fewer wines of poorer quality 3 and 4 and good quality of 7 and 8 classified at the lower and upper tails, respectively :
```{r histogram, echo=FALSE, include=FALSE}

distribquality <- ggplot(winequality_nd, aes(x=quality))+geom_histogram(bins=20)+geom_vline(data=winequality, aes(xintercept=mean(quality)), col="red")+geom_vline(data=winequality, aes(xintercept=median(quality)), col="green")+scale_x_continuous(breaks=c(1:10))+ggtitle("distribution of wine quality; green line= median, red line= average quality")

```
```{r, echo=FALSE}

distribquality
```


In later sections, where classification models are used to predict a ‘good’ wine (wines with a quality rank of 7 or more) versus a ‘poor’ wine (wines with a quality rank of 6 or less), a class imbalance is in evidence.  It is evident, looking at the histogram that wines ranked as ‘good’ are far outnumbered by wines ranked as ‘poor’.  The class imbalance should be acknowledged when estimating and evaluating the models.  Further discussion on this matter is presented later in this report.

In general terms, the database is described as having moments as described in Table 2 for the chemical components and the quality descriptor of the wines:

```{r moments1, echo=FALSE, include=FALSE}

###describe the variables by mean, median, skewness, min and max:
mean <- as.matrix(format(colMeans(winequality_nd), digits=1))
median <- as.matrix(format(colMedians(as.matrix(winequality_nd)),digits=1))
maximum <- as.matrix(format(colMaxs(as.matrix(winequality_nd)),digits=1))
minimum <- as.matrix(format(colMins(as.matrix(winequality_nd)),digits=1))

#maximum
#minimum
#median
#mean
stats <- cbind(mean, median,minimum, maximum)
colnames(stats) <- c("mean", "median", "min", "max")
stats<- noquote(stats)

```

```{r , echo=FALSE}

kable(stats, caption="Moments of wine quality database, duplicates removed", "latex", booktabs=TRUE, align = "rrrr")
```



The mean of the distribution of quality in the wine database is 5.62.  The median is located to the right of the average rating at 6.  As the mean of the distribution is less than the median, the distribution is negatively skewed. 

The chemical composition variables were scaled to a z-distribution, to permit reliable estimations of the relations and contributions of each to the prediction models.   The quality variable is not scaled.  The distributions of the scaled variables are shown in Table 3.

```{r table3, echo=FALSE, include=FALSE}

##standardize the variables contributing to wine quality
#function for standardizing to z distribution: 
mean_sd_standard <- function(x) {
  (x-mean(x))/sd(x)
}
# Apply the function to each numeric variable in the data set
winequality_standardized <- winequality_nd %>% mutate(quality=as.factor(quality)) %>%
  mutate_if(is.numeric, mean_sd_standard)
str(winequality_standardized)

winequality_standardizedA <- winequality_standardized %>% mutate(quality=as.numeric(as.character(quality)))
#Summarize the standardized data
table_3 <- summary(winequality_standardizedA)

mean2 <- as.matrix(round(colMeans(winequality_standardizedA), digits=3))
median2 <- as.matrix(format(colMedians(as.matrix(winequality_standardizedA)),digits=1))
maximum2 <- as.matrix(format(colMaxs(as.matrix(winequality_standardizedA)),digits=3))
minimum2 <- as.matrix(format(colMins(as.matrix(winequality_standardizedA)),digits=3))

#maximum2
#minimum2
#median2
#mean2
stats2 <- cbind(mean2, median2,minimum2, maximum2)
colnames(stats2) <- c("mean", "median", "min", "max")
stats2a <-noquote(stats2[c(1:11),])


```



```{r stats2, echo=FALSE}

kable(noquote(format(stats2a, digits=1, nsmall=2)),caption="Moments of standardized wine quality database, duplicates removed ", "latex", align = "rrrr", booktabs=TRUE)
```


Understanding the database includes an understanding of the explanatory variables:  the explanatory variables are chemical components that have been selected in this model to be representative of wine quality. But other studies of chemical analysis of wine are looking at a analyses that do not necessarily replicate the list offered in this database.     The interaction of the chemical components in the wine is not perfectly understood; otherwise, chemists would be designing wines to perfection.  There would be no need for tasting wines.  But, for a data scientist there remains a problem:  should one or any of the chemical components in the database should be removed? As I explain later on, data analysis does not easily reveal which of the chemical explanatory variables is ‘redundant’ and therefore easily removed to improve a model. 

Corrplot is used to describe the correlation patterns between the chemical composition variables and the subjective quality variable (which, for correlation analysis, remains in the range between 3 to 8):

```{r correlation1, include=FALSE, echo=FALSE}

##rearrange the columns so that the quality column is first
winequalitystandardized <- winequality_standardized %>% select(quality, everything()) 

##try another correlogram form

##the data in this procedure must be numeric
winequality_numeric <- winequalitystandardized %>% mutate(quality=as.numeric(quality))

# Use of the winequality_standardized data proposed by R...wine quality is a numeric (not standardized)
winedata <- cor(winequality_numeric)
#winedata
# Build a Pannel of 100 colors with Rcolor Brewer
my_colors <- brewer.pal(5, "Spectral")
my_colors <- colorRampPalette(my_colors)(200)

##create corellogram......this is the database of no duplicates, not standardized, quality is a number
figure_2 <- ggcorr(winedata, method = c("everything", "pearson"), hjust = 0.8, size = 3, nbreaks = 4, palette = "RdGy", label = TRUE, label_size = 3, label_color = "white")  + ggplot2::labs(title = "Correlation estimates (Pearson method) of wine variables")

```

```{r figure_2, echo=FALSE}

figure_2

```

As can be observed in above correlation diagram, there are a few very strong correlations (as indicated by the red and  black coloured squares) and many weaker relationships between the variables. Noted is that alcohol is showing strong linear correlation to the quality measures, with sulphates showing correlation to a lesser degree. The correlation diagram below indicates which of the correlations between the variables are statistically significant; blank spaces indicate no statistical significance. The presence of significant correlations between explanatory variables might indicate that multicollinearity would be a problem in models created to identify ‘good’ wines.  

```{r figure3,  echo=TRUE}

 ##keep##significance level for correlation is at 0.01 for p.mat
testRes <- cor.mtest(winequality_numeric, conf.level = 0.95)

Fig3_winequality <- corrplot(cor(winequality_numeric), type="upper", order="FPC", 
                    p.mat=testRes$p, sig.level = 0.01, insig='blank',
                    title = "Correlations - ordered by first principal component",
                    mar=c(0,0,.75,0), number.cex = .7)

```

Principal component analysis is used to describe which of the variables are contributing the most towards describing the variance in a classification model, so that such variables could be dropped to reduce multicollinearity or redundancy in the data. 

Principal component analysis is carried out using the R packages FactoMineR and factoextra^[http://www.stdth.com/english/rpkgs/factoextra as described in analytical methods presented in Practical Guide to Principal Component Methods in R, Alboukadel Kassambara, 2017].  Many authors have described the mathematics of principal component analysis and the interpretation of the eigenvalues and eigenvectors produced^[A nice chatty discussion is found at https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues], ^[https://en.wikipedia.org/wiki/Principal_component_analysis].  In this paper, my purpose is limited to identifying the variables which are most likely to contribute to identification of a good wine.  However, principal component analysis (as modelled in this paper) is limited to an understanding as it relates to linear relationships between the variables;  I suspect that the chemical components of wine may not be easily explained by linear equations and therefore must view the principal component analysis results with caution.

Graphing the PCA components reveals that the first component explains 28 % of the variance while the second component explains 17% of the variance, representing 45% of the information of the original dataset. Variables positively correlated as they contribute to the principal components, are grouped together and negatively correlated variables are on the opposed quadrants.  The colours of the variable vectors indicate their contribution to the principal components (principal component 1 and principal component 2).  A higher percent indicates a greater contribution to the Principal component, towards explaining variance in the dependent variable.

```{r , echo=FALSE, include=FALSE}
##set the working directory

winequalitystandardized2G<-winequalitystandardized%>%mutate(quality=as.numeric(as.character(quality)),group=ifelse(quality > 6,"good","poor"))
winequal1 <- as.factor(winequalitystandardized2G[,13])
winequal2 <- cbind(winequal1, winequalitystandardized2G[,2:12])
##Reduce the dimensions with Principal Component Analysis
winequality_pca <- PCA(winequal2[,2:12])
## the results indicate that the first two principal components explain 28% and 17% of the variance.
##sum the variance preserve by th first two component
variance_first_two_pca <- winequality_pca$eig[1,2] + winequality_pca$eig[2,2]

```

```{r, echo=FALSE}
##set the working directory

##keep###visualize the variables on the grid - contribution to pc1 and pc2
contrib_var <- fviz_pca_var(winequality_pca, col.var="contrib", gradient.cols=c("#00AFBB", "#E7B800","#FC4E07"))
contrib_var
```
Looking at all dimensions of the principal components, we look at the scree plot of the principal component dimensions to find the contribution (toward explaining the variance in the wine quality) of each dimension.  Of course, the contribution of each successive dimension diminishes:

```{r, echo=FALSE, include=FALSE}

##set the working directory

###examine the eigenvalues (which measure the amount of variation retained by each principal component)
eig.val <- get_eigenvalue(winequality_pca)
eig.val
##the proportion of variation explained by each eigenvalue is given in the second column
##to look at patterns in the data, a cumulative.variance.percent of 70% is considered satisfactory
##in this case, the first four or five eigenvalues (principal components)
##explain 80% of the variance

##keep##a scree plot presents the eigenvalues in a plot
scree_plot_eigenvalues <- fviz_eig(winequality_pca, addlabels=TRUE, ylim=c(0,30))

```
```{r, echo=FALSE}
##set the working directory

scree_plot_eigenvalues 



```
Kassambara (2017)^[in Practical Guide to Principal Component Methods in R, Alboukadel Kassambara, 2017, Section 3.4.2.4 Contributions of variables to PCs, page 24.]  states that the contribution of variables in accounting for variability of principal components can be expressed as a percentage, whereby variables that are correlated with PC1 (in dimension 1) and PC2 (in dimension 2) are the most important in explaining the variability in the dataset.  Variables that are not well correlated with any principal component or which are correlated only with the last dimension principal components are variables with low contribution and may (under the right circumstances) be removed to simplify the analysis. In the diagram above (Variables – PCA), the variables of low contribution are represented by a short vector towards the circumference of the circle representing the first two dimensions.  The variables closer to the circumference may be more important in explaining the variability in the dataset.  These concepts are represented again below.  The contribution (in percentages) for a variable is described for the first two dimensions.  For the first dimension, the top contributors are fixed.acidity, citric.acid, pH, and density.  The red line on the illustration indicates the expected average contribution of the variables, if the contribution of each variable were uniform. 

```{r, echo=FALSE, include=FALSE}

one <- fviz_contrib(winequality_pca, choice="var", axes=1, top =10)
two <- fviz_contrib(winequality_pca, choice="var", axes=2, top=10)

###graph the top contributors to each dimension
#topcontrib <- fviz_contrib(winequality_pca, choice="var", axes=1:2, top=10)
#topcontrib

##keep#title <- "Contributors to each dimension"
var_wine <- get_pca_var(winequality_pca)


```


```{r, echo=FALSE,fig.align = 'center' }
one

```
We look at the top contributors to the second dimension, below:
```{r, echo=FALSE, fig.align = 'center'}
two


```
Viewing the contribution of each variable to the top five dimensions:
```{r, echo=FALSE, fig.align = 'center'}
corrplott <-corrplot(var_wine$contrib, is.corr=FALSE, col = COL2('RdBu', 10))

```
In dimension 2, we see a shift in the contribution of the variables:  total.sulfur.dioxide, free.sulfur.dioxide and alcohol contribute above the expected average contribution.  If we look at the contributors in in each of the dimensions, we see that chlorides and sulphates are consistently below the expected average contribution.  Recalling from the PCA graph of variables that 1)  the contribution of chlorides (to explaining variance in the dataset) are highly correlated with density and 2) the contribution of sulphates are highly correlated with fixed.acidity.  Should these two variables be dropped from the classification models in hopes of eliminating ‘noise’?  We see in the above diagram that chlorides and sulphates are the highest contributors to dimension 4;  we can also see that dimension 4 contributes only  11% towards explaining the variance in the model (a similar line of reasoning applied when viewing the contribution of residual.sugar in dimension 5).  If the contributions of variables in the dimensions further from the first orthogonals are weak, are they merely contributing to redundancy in the model and do we gain by including these variables in a model? (no answers yet…but, in later sections we find that alcohol is the leading contributor to our models and that sulphates are often ranked second in importance).  

Of course, there remains possible complications from correlations between other variables which will kept in mind when evaluating the classification models. Reviewing the correlation coefficients as estimated using corrplot, sulphates and chloride columns were not showing exceptional correlation estimates with each other; sulphates showed weak correlation to quality.  The highly correlated variables, including total.sulfur.dioxide and free.sulfur.dioxide show in PCA analysis as good contributors towards explaining the variance in the dataset.   But, this is as expected: Variance does not capture the inter-column relationships or the correlation between variables .  Underlying all analyses of correlation and of the principal components is the assumption that the relationships between the explanatory variables is linear and that these linear relationships can be rearranged to be represented by the linear correlation and principal component functions.  If the relationship between a few or many of the variables in this model are non linear, then linear analytical tools may be misleading.

Finally, we take a look at the distribution of the wine quality groups (group 1 is all wines with quality assignments greater than or equal to 7;  group 0 is all other quality groups) on the principal component dimensions 1 and 2:  

```{r , echo=FALSE, include=FALSE}
#assess ability to cluster....shows scatter of quality groups
clustdf <- winequalitystandardized[,2:12]
#str(clustdf)

##keep# Plot grouped quality data set####group good versus group poor
tendency <- fviz_pca_ind(prcomp(clustdf), title = "PCA - wine data", 
                         habillage = winequal2$winequal1,  palette = "jco",
                         geom = "point", ggtheme = theme_classic(),
                         legend = "bottom")


```
Assessing the distribution over the dimensions, it seems possible to identify groups of good quality wines (blue dots) versus the poor quality wines (yellow triangles).  There is overlap in evidence across the two dimensions.  However, the good wines seem to occupy the dimension 1 quadrant with a greater presence than the poor wines.  Also in evidence are the outliers – which were not removed from the sample but perhaps should be if a linear model is preferred.  However, these outliers are showing up in a mapping of the first two principal components. What is identified as an outlier on this diagram may not be identified as such in a multidimensional space, which I do not attempt to illustrate.
  

```{r, echo=FALSE, fig.align = 'center'}

tendency

```


  
  
## **The Classification Models**
  
I expect, looking at the correlation analysis and the principal component analysis, the model describing the classification of wines between poor and good may be nonlinear.  I decided to first try a cross validated logistic regression classification model expecting this classification model to perform less well than other models.  I also attempt a random forest classification model and a gradient boosted algorithm classification model.   
The purpose of all classification models is to identify a ‘good’ wine, as identified by a quality rank of 7 or more.  
  
In the red wine data set we are working with, we know that ‘good’ wines (as identified by a quality of 7 or over) are far outnumbered by ‘poor’ wines (identified by a quality of 6 and under): there are 1175 poor wines and 184 good wines. The dataset has been randomly split into the train set (80% of the total dataset with 145 good wines and 942 poor wines) and the test set (20% of the total data set with 39 good wines and 233 poor wines).   After creation, the same train set and the test set were used in all models in this report.  For the logistic model and the gradient boosted models, the train set was subsequently reweighted (from itself) and adjusted to account for class imbalance, as poor wines clearly outnumber good wines in all data sets. The test set remains in its original form (39 good wines, 233 poor wines).  As the test sets are the same across all models, comparisons of the performance of the models are easily interpreted if I decide that my models are well defined (which is questionable, once I try to understand everything there is to know about the caret package and to explain all the metrics and tuning of model parameters, beginning with the train control functions).
  
### **The Logistic Model**
  
The logistic classification model was created to predict a classification of wine to good or poor based upon the chemical components in the red wine database.  A cross validated model was used, as cross validation is recommended as the best modelling structure to avoid misrepresentation of the actual relationships due to a ‘lucky’ sample of the population of regressors^[https://rafalab.github.io/dsbook/cross-validation.html] .  Cross validation is an attempt to make use of repeated sampling from a subset of a population, to create a better portrait of the unseen population.  As there is some evidence that a class imbalance exists in the data set (where the good wines are far outnumbered by poor wines in the sample), I include a logistic regression where the train set was resampled using ‘down’ sampling (the count of poor wines was reduced to equal the count of good wines in the train set).
  
The cross validated logistic classification model was created using the train and predict functions of the caret package. 
  

```{r, echo=FALSE, include=FALSE}

##set a seedvalue to set the seed

set.seed(5627)


##first up, create the dataset with the groups and the contributing chemical descriptors
str(winequal2)
winequalcount <- table(winequal2$winequal1)
winequalcount
#create test and train datasets....use winequal2 for the first run of cross validated logistic regression
split_dummy <- sample(c(rep(0, 0.8 * nrow(winequal2)),  # Create dummy for splitting
                        rep(1, 0.2 * nrow(winequal2))))
table(split_dummy) 

data_train2 <- winequal2[split_dummy == 0, ]             # Create train data with all variables
data_test2 <- winequal2[split_dummy == 1, ]      #Create test data with all variables
str(data_train2)
str(data_test2)
winequalcounttrain <-table(data_train2$winequal1)
winequalcounttrain
winequalcounttest <- table(data_test2$winequal1)
winequalcounttest
######Part 1#### the cross validated logistic regressions

###the cross validation model for the logistic regression with all variables..
```
  
```{r, echo=TRUE}
cntrlspecs_cv <- trainControl(method="cv", 
                              number=10, 
                              savePredictions="all", 
                              classProbs=TRUE)


set.seed(5627)

logistic_model_cv <- train(winequal1 ~ ., 
                           data=data_train2, 
                           method="glm", 
                           family="binomial", 
                           trControl=cntrlspecs_cv)
```
  
Tests were conducted to determine whether multicollinearity should be considered as a problem in the models. The tests were conducted using the multiColl package.  The results of the test shown indicate the variable inflation factor (VIF) of each independent variable in the dataset.  The rule of thumb is that the VIF should not exceed 10.  The VIF of the variables does not indicate a variance inflation factor greater than 10 for any of the chemical components, and therefore adjustments to correct the classification models for multicollinearity are not productive.  However, it is noted that fixed.acidity has a VIF of 8.13 and density is at 6.33; these measures are a little high.  For the most part, the VIF measures remain well under 5.  Multicollinearity, by the measures available, does not seem to be a serious issue.
  
```{r, echo=FALSE, include=FALSE}

#print(logistic_model_cv)
#print.train(logistic_model_cv)
summary(logistic_model_cv)
logistic_model_cv



str(data_train2)
head(data_train2)
cte <- array(1,length(data_train2[,1]))
train2.X <- cbind(cte,data_train2[,-(1)])

CN.test <-CN(train2.X)
CN.test
####Values of CN between 20 and 30 indicate near moderate multicollinearity while values higher than
###30 indicate near worrying collinearity. https://cran.r-project.org/web/packages/multiColl/multiColl.pdf
CN.test.var <- CNs(train2.X)
CN.test.var

ki.test <- ki(train2.X)
ki.test

multiCol.test <- multiCol(train2.X, dummy=FALSE, pos=NULL)
multiCol.test
VIFmeasures <- multiCol.test$`Variance Inflation Factors`


```
  
```{r, echo=FALSE, fig.align = 'center'}
kable(noquote(format(VIFmeasures, digits=2, nsmall=2)), caption="VIF measures of multicollinearity", col.names=NULL, align='r', "latex")
```
  
```{r thelogisticmodel, echo=FALSE, include=FALSE}

#####all tests indicate that multicollinearity is not a problem

###variable importance
var_logistic_model_cv <- varImp(logistic_model_cv)
var_logistic_model_cv

###create the variable importance table
Variable_importance_models <- tibble(method = "logistic", sulphates=100, alcohol=68.553, volatile.acidity=52.120,
                                     fixed.acidity=44.430, density=35.858, total.sulfur.dioxide=35.786, residual.sugar=33.608,
                                     chlorides=28.272, pH=10.019, citric.acid=3.141, free.sulfur.dioxide=0.00)

predict_logistic_model_cv <- predict(logistic_model_cv, newdata=data_test2)
predict_logistic_model_cv

###now, do the confusion matrix

###put the predicted values and the true values into a table
table_cv <- table(predict_logistic_model_cv, data_test2$winequal1)
#table_cv


recalllm <- table_cv[1,1]/(table_cv[1,1]+table_cv[2,1])
precisionlm <- table_cv[1,1]/(table_cv[1,1]+table_cv[1,2])
f1lm <- 2/((1/precisionlm)+(1/recalllm))
#precisionlm
#recalllm
#f1lm
###test the predictions in confusion matrix
resultslogisticcv <- confusionMatrix(table_cv)
#resultslogisticcv
str(resultslogisticcv)

####take apart the confusion matrix
gettable <- as.data.frame(resultslogisticcv$table)
#gettable
true_good <- gettable[1,3]
false_good <- gettable[3,3]
false_poor <- gettable[2,3]
true_poor <- gettable[4,3]

misclassificationlm <- (table_cv[1,2]+table_cv[2,1])/(table_cv[1,1]+table_cv[1,2]+table_cv[2,1]+table_cv[2,2])
#misclassificationlm

getKappa <- as.data.frame(resultslogisticcv$overall)
Kappacv <- getKappa[2,]
#Kappacv
Kappa_results <- tibble(method = "logistic", Kappa= Kappacv,
                        Recall=recalllm, Precision = precisionlm, F1 = f1lm, misclassification = misclassificationlm, 
                        true_good=true_good, false_good=false_good, false_poor=false_poor, true_poor=true_poor)



###create the confusion matrix for the precision recall indicators
lm_pr <- confusionMatrix(table_cv, mode = "prec_recall")
#lm_pr
#####testing something####leaving it here because i am testing code
##tableup2 <- table(predict_logistic_model_cv,data_test2$winequal1)
##tableup2
##recallup2 <- tableup2[1,1]/(tableup2[1,1]+tableup2[2,1])
##precisionup2 <- tableup2[1,1]/(tableup2[1,1]+tableup2[1,2])
##f1up2 <- 2/((1/precisionup2)+(1/recallup2))
##precisionup2
##recallup2
##f1up2
##observed from the above, sulphates, alcohol, density, total.sulfur.dioxide, chlorides, residual.sugar. volatile.acidity, fixed.acidity are
###all significant contributors (at varying levels of significance)
##accuracy seems pretty good.  but the kappa indicates that guessing is whether a wine is good or not is better than the model...so, why bother with a model?
####https://www.r-bloggers.com/2019/12/how-to-make-a-precision-recall-curve-in-r/




#####
####
####create the precision recall curve

## run MLeval
x <- evalm(list(logistic_model_cv))
## curves and metrics are in the 'x' list object

#######does class imbalance affect the results of the logistic model?
###the model may suffer from class bias, as there are more group 0 than group 1 in the data_train set
table(data_train2$winequal1)
## to resolve the class bias, resample such that the group 0 and group 1 are in similar proportions in the train set

####try downsampling....taking the train set and removing some of the 'poor' wines to create a balanced ratio of good to poor wines
#####test set remains unaffected
poor <- which(data_train2$winequal1 == "poor")
good <- which(data_train2$winequal1 == "good")
length(poor)
length(good)
poor.downsample <- sample(poor, length(good)) 
#str(poor.downsample)
newtrainpoor <- data_train2[c(poor.downsample),]
#str(newtrainpoor)
newtraingood <- data_train2[c(good),]
#str(newtraingood)
newtraindown <- rbind(newtraingood, newtrainpoor)
#str(newtraindown)

###the cross validation model for the logistic regression with all variables..
cntrlspecs_cv <- trainControl(method="cv", number=10, savePredictions="all", classProbs=TRUE)


set.seed(5627)

logistic_model_cvdown <- train(winequal1 ~ ., data=newtraindown, method="glm", family="binomial", trControl=cntrlspecs_cv)

print(logistic_model_cvdown)
print.train(logistic_model_cvdown)
summary(logistic_model_cvdown)
#logistic_model_cvdown

######look at the results of the logistic regression ....recall and precision from confusion matrix
###variable importance
var_logistic_model_cvdown <- varImp(logistic_model_cvdown)
#var_logistic_model_cvdown

###create the variable importance table
Variable_importance_models <- tibble(method = "logistic - down sample", sulphates=100, alcohol=68.553, volatile.acidity=52.120,
                                     fixed.acidity=44.430, density=35.858, total.sulfur.dioxide=35.786, residual.sugar=33.608,
                                     chlorides=28.272, pH=10.019, citric.acid=3.141, free.sulfur.dioxide=0.00)

predict_logistic_model_cvdown <- predict(logistic_model_cvdown, newdata=data_test2)
#predict_logistic_model_cvdown

###now, do the confusion matrix

###put the predicted values and the true values into a table
table_cvdown <- table(predict_logistic_model_cvdown, data_test2$winequal1)
#table_cvdown


recalllmdown <- table_cvdown[1,1]/(table_cvdown[1,1]+table_cvdown[2,1])
precisionlmdown <- table_cvdown[1,1]/(table_cvdown[1,1]+table_cvdown[1,2])
f1lmdown <- 2/((1/precisionlmdown)+(1/recalllmdown))
#precisionlmdown
#recalllmdown
#f1lmdown
###test the predictions in confusion matrix
resultslogisticcvdown <- confusionMatrix(table_cvdown)
#resultslogisticcvdown
#str(resultslogisticcvdown)

####take apart the confusion matrix
gettabledown <- as.data.frame(resultslogisticcvdown$table)
gettabledown
true_good <- gettabledown[1,3]
false_good <- gettabledown[3,3]
false_poor <- gettabledown[2,3]
true_poor <- gettabledown[4,3]

misclassificationlmdown <- (table_cvdown[1,2]+table_cvdown[2,1])/(table_cvdown[1,1]+table_cvdown[1,2]+table_cvdown[2,1]+table_cvdown[2,2])
#misclassificationlmdown

getKappa <- as.data.frame(resultslogisticcvdown$overall)
Kappacvdown <- getKappa[2,]
#Kappacvdown
Kappa_results <- add_row(Kappa_results,method = "logistic-down sample", Kappa= Kappacvdown,
                         Recall=recalllmdown, Precision = precisionlmdown, F1 = f1lmdown, misclassification = misclassificationlmdown, 
                         true_good=true_good, false_good=false_good, false_poor=false_poor, true_poor=true_poor)



###create the confusion matrix for the precision recall indicators
lm_prdown <- confusionMatrix(table_cvdown, mode = "prec_recall")
#lm_prdown


## run MLeval
comparelogisticmodels <- evalm(list(logistic_model_cv, logistic_model_cvdown))
#comparelogisticmodels$roc  ###to be included under the results section
```
  
  
### **The Random Forest Model**
  
Random forest models are a learning tool for classification^[https://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/] ^[https://www.listendata.com/2014/11/random-forest-with-r.html].   A random forest is a supervised machine learning algorithm that is constructed from decision tree algorithms ^[https://www.section.io/engineering-education/introduction-to-random-forest-in-machine-learning/] . It can be more accurate than a decision tree algorithm as it takes iterations of many decision trees before an optimal classification algorithm is obtained, without the overfitting encountered by decision trees. Random forests are forgiving when encountering outliers ^[https://datascience.stackexchange.com/questions/54751/when-to-use-random-forest].   
  
The random forest algorithm was tuned to find the best mtry (the number of predictors used in the decision node of each tree), number of trees (Ntree), node size (per Probst et at (2019): The nodesize parameter specifies the minimum number of observations in a terminal node. Setting it lower leads to trees with a larger depth which means that more splits are performed until the terminal nodes) and the best decision for maximum nodes of the trees (the maximum number of terminal nodes that the trees in the forest can have).  Cross validation was used to select these features (with the caveat:  as if I were fully aware of what I was doing to the model). The tuning of the random forest model was done using the caret package for train functions.   Noted from a paper by Probst et al (2019)^[https://arxiv.org/pdf/1804.03515.pdf] : 
  
>“Note that the convergence rate of RF does not only depend on the considered dataset’s characteristics but possibly also on hyperparameters. Lower sample size (see Section 2.1.2), higher node size values (see Section 2.1.3) and smaller mtry values (see Section 2.1.1) lead to less correlated trees. These trees are more different from each other and are expected to provide more different predictions. Therefore, we suppose that more trees are needed to get clear predictions for each observation which leads to a higher number of trees for obtaining convergence.” 
  
After reading Probst et al, I decided to go with a model with a higher nodesize (at 8) and for a higher number of trees (450) than my (inexpert) tuning exercises indicated.   To be noted:  if I ‘tune’ mtry and then nodesize and then number of trees, is number of trees optimal if there were unknown interactions between the first test on mytry and the second tuning test on nodesize?  I ran my final random forest model using a tuning grid to select the best mtry.  I also discovered that ‘tuning’ my model was ‘over training’ because as I tuned, without really understanding all the interactions between the metrics, I get what I identified as ‘the best’ random forest model.  And then proceeding further, I created a tune free nodesize/tune free maxmode/tune free ntree model, to have a look at what all my tuning had wrought. Apparently, increasing the node size has the model select more good wines, but also has the model falsely identifying the poor wines in close proximity (to good wines) as good wines.   I am still not sure what the effects of mtry and ntrees have on the outcome of the model (I don’t have time to test everything; that would take another project). 
  
```{r, randomforestmodel, echo=FALSE, include=FALSE}
# curves and metrics are in the 'x' list object

###the Random Forest model to predict a wine in group 1 from the chemical components 
###the reference for the Random Forest model is https://www.guru99.com/r-random-forest-tutorial.html  and https://rpubs.com/Mentors_Ubiqum/tunegrid_tunelength
### a detailed discussion for 'tuning' mtry, nodesize and sample size is found under https://arxiv.org/pdf/1804.03515.pdf
#####from the article above:  Note that the convergence rate of RF does not only depend on the considered dataset's characteristics but possibly
##also on hyperparameters. Lower sample size (see Section 2.1.2), higher node size values (see Section 2.1.3) and smaller
##mtry values (see Section 2.1.1) lead to less correlated trees. These trees are more different from each other and
##are expected to provide more different predictions. Therefore, we suppose that more trees are needed to get clear
##predictions for each observation which leads to a higher number of trees for obtaining convergence.




###the cross validated model is to select the best mtry, the best number of trees to select, the best number of nodes
###the cross validated selection for the best model is done by a grid pattern, as it is easiest to understand
###the random forest models tested are on the full set of variables as described in the dataset winequal2

#####use class weights to correct for class imbalance
#####now, bring in the weights, to see if 'sample up' or 'sample down' would improve the results
# Create model weights for the random forest model, to correct for class imbalance (they sum to one)
####not required for report...no improvements in the model
#model_weights <- ifelse(data_train2$winequal1 == "poor", (1/table(data_train2$winequal1)[2])*.5, (1/table(data_train2$winequal1)[1])*.5)
#model_weights



#####The random forest models....####this first section has been used to 'tune' the model for mtry, nodesize, maxnodes and ntree....
# Define the control
tuneGrid <- expand.grid(.mtry = c(1: 10))
set.seed(5627)
trControlRF <- trainControl(method = "cv",
                            number = 10,
                            search = "grid",
                            classProbs=TRUE)

rf_wine_default <- train(winequal1 ~ ., data=data_train2,
                         metric="Accuracy",
                         tuneGrid = tuneGrid,
                         trControl=trControlRF,
                         importance=TRUE,
                         nodesize = 14,
                         ntree = 300)
#-The first parameter specifies our formula: winequal1 ~ . (we want to predict wine quality using each of the remaining columns of data).
#ntree defines the number of trees to be generated. It is typical to test a range of values for this parameter (i.e. 100,200,300,400,500) and choose the one that minimises the OOB estimate of error rate.
#mtry is the number of features used in the construction of each tree.  The default value for this parameter, when performing classification, is sqrt(number of features/variables).
#importance enables the algorithm to calculate variable importance
results_rf_default <- rf_wine_default
results_rf_default




###store the best mtry for Accuracy
(bestmtry <- rf_wine_default$bestTune$mtry)
(accuracywbestmtry <- max(rf_wine_default$results$Accuracy))

##
###find the best number of nodes in the random forest model, using bestmtry
store_maxnode <- list()
tuneGrid <- expand.grid(.mtry = bestmtry)
for (maxnodes in c(8: 20)) {
  set.seed(5627)
  rf_maxnode <- train(winequal1~.,
                      data = data_train2,
                      method = "rf",
                      metric = "Accuracy",
                      tuneGrid = tuneGrid,
                      trControl = trControlRF,
                      # weights = model_weights,#
                      importance = TRUE,
                      nodesize = 8,
                      maxnodes = maxnodes,
                      ntree = 450)
  current_iteration <- toString(maxnodes)
  store_maxnode[[current_iteration]] <- rf_maxnode
}
results_mtry <- resamples(store_maxnode)
best_nodes <- summary(results_mtry)
(best_nodes)
#if i go for the best accuracy and the best kappa combination, i think the optimal number of nodes is 8 or more (best accuracy and best kappa)
###the program describes recommends taking the best nodes and then setting a maximum nodes to tune for the optimal number of trees

store_maxtrees <- list()
for (ntree in c(250, 300, 350, 400, 450, 500, 550, 600, 800)) {
  set.seed(5627)
  rf_maxtrees <- train(winequal1~.,
                       data = data_train2,
                       method = "rf",
                       metric = "Accuracy",
                       tuneGrid = tuneGrid,
                       trControl = trControlRF,
                       # weights = model_weights,#
                       importance = TRUE,
                       nodesize = 8,
                       maxnodes = 18,
                       ntree = ntree)
  key <- toString(ntree)
  store_maxtrees[[key]] <- rf_maxtrees
}
results_tree <- resamples(store_maxtrees)
summary(results_tree)
####put in the best tuned parameters for the random forest model
#####lots of work done to test the parameters of the random forest model
####but all that has been revealed is that mytry is rule of thumb, nodes-12-24, ntrees=300
#
####the results indicate that there is no improvement to accuracy or kappa by increasing the number of trees past 250
###max accuracy is at 300 trees;  max kappa is also at 300 trees (except that i ran the rf model and got better results with 450 trees)
###optimal max number of nodes is 18 ;
##optimal mtry is 3


# Define the control
#tuneGrid <- expand.grid(.mtry = c(1: 10))
```
  
The random forest model, with tuned metrics for nodesize, maxnodes, ntree  and mtry on a tune grid from 1 to 10:
  
```{r, randomforestmodeltuned}
set.seed(5627)
tuneGrid <- expand.grid(.mtry = c(1: 10))
trControlRF <- trainControl(method = "cv",
                            number = 10,
                            search = "grid",
                            classProbs=TRUE,
                            summaryFunction=twoClassSummary)

rf_wine <- train(winequal1 ~ ., data=data_train2,
                 metric="ROC",
                 tuneGrid = tuneGrid,
                 trControl=trControlRF,
                 importance=TRUE,
                 nodesize = 8,
                 maxnodes=19,
                 ntree = 450)

```
  
```{r randomforestnt, echo=FALSE, include=FALSE}

summary(rf_wine)


var_rfmodel <- varImp(rf_wine)
var_rfmodel
Variable_importance_models <- add_row(Variable_importance_models,method = "random forest", sulphates=74.62, alcohol=100.00, volatile.acidity=42.61,
                                      fixed.acidity=27.50, density=30.91, total.sulfur.dioxide=24.60, residual.sugar=0.00,
                                      chlorides=5.96, pH=10.43, citric.acid=13.42, free.sulfur.dioxide=11.09)

predict_rfmodel <- predict(rf_wine, newdata=data_test2)
#predict_rfmodel

###put the predicted values and the true values into a table
table_rf <- table(predict_rfmodel, data_test2$winequal1)



###create the confusion matrix for the precision recall indicators
rf_pr <- confusionMatrix(table_rf, mode = "prec_recall")
rf_pr


###test the predictions in confusion matrix
resultsrfmodel <- confusionMatrix(table_rf)
getKappa <- as.data.frame(resultsrfmodel$overall)
Kapparf <- getKappa[2,]


tablerfc <- table(predict_rfmodel,data_test2$winequal1)
tablerfc
recallrfc <- tablerfc[1,1]/(tablerfc[1,1]+tablerfc[2,1])
precisionrfc <- tablerfc[1,1]/(tablerfc[1,1]+tablerfc[1,2])
f1rfc <- 2/((1/precisionrfc)+(1/recallrfc))
precisionrfc
recallrfc
f1rfc
misclassificationrf <- (tablerfc[1,2]+tablerfc[2,1])/(tablerfc[1,1]+tablerfc[1,2]+tablerfc[2,1]+tablerfc[2,2])
misclassificationrf
gettablerf <- as.data.frame(resultsrfmodel$table)
gettablerf
true_goodrf <- gettablerf[1,3]
false_goodrf <- gettablerf[3,3]
false_poorrf <- gettablerf[2,3]
true_poorrf <- gettablerf[4,3]

Kappa_results <- add_row(Kappa_results, method = "random forest", Kappa= Kapparf,
                         Recall=recallrfc, Precision = precisionrfc, F1 = f1rfc, misclassification = misclassificationrf,
                         true_good=true_goodrf, false_good=false_goodrf, false_poor=false_poorrf, true_poor=true_poorrf)




#####
####


#Build custom AUC function to extract AUC
# from the caret model object
###this is a function created here only as a test...to ensure that i can put 
#the data into the martin model later... to put the data in (to create various models adjusting for bias, later)
test_roc <- function(model, data) {
  roc(data$winequal1,predict(model, data, type = "prob")[, "good"])
}

random_forest_auc <- rf_wine %>%test_roc(data = data_test2) %>% auc()
random_forest_auc

```
  
The random forest model with 'no tuning' of nodesize, maxnodes or ntree, with mtry on a tune grid from 1 to 10:
  
```{r randomforestnottuned}
set.seed(5627)
tuneGrid <- expand.grid(.mtry = c(1: 10))
trControlRF2 <- trainControl(method = "cv",
                             number = 10,
                             search = "grid",
                             classProbs=TRUE,
                             summaryFunction=twoClassSummary)

rf_wineNT <- train(winequal1 ~ ., data=data_train2,
                   metric="ROC",
                   tuneGrid = tuneGrid,
                   trControl=trControlRF2,
                   importance=TRUE,
                   #nodesize = 8,#
                   #maxnodes=19,#
                   #ntree = 450#)
)
```
  
```{r randomforestnottunedoutput, echo=FALSE, include=FALSE}

summary(rf_wineNT)


var_rfmodelNT <- varImp(rf_wineNT)
var_rfmodelNT

Variable_importance_models <- add_row(Variable_importance_models,method = "random forest Not Tuned", sulphates=84.820, alcohol=100.00, volatile.acidity=58.022,
                                      fixed.acidity=31.813, density=46.174, total.sulfur.dioxide=46.953, residual.sugar=0.00,
                                      chlorides=34.441, pH=6.084, citric.acid=25.752, free.sulfur.dioxide=2.101)



predict_rfmodelNT <- predict(rf_wineNT, newdata=data_test2)
#predict_rfmodel

###put the predicted values and the true values into a table
table_rfNT <- table(predict_rfmodelNT, data_test2$winequal1)



###create the confusion matrix for the precision recall indicators
rf_prNT <- confusionMatrix(table_rfNT, mode = "prec_recall")
rf_prNT


###test the predictions in confusion matrix
resultsrfmodelNT <- confusionMatrix(table_rfNT)
getKappa <- as.data.frame(resultsrfmodel$overall)
KapparfNT <- getKappa[2,]


tablerfcNT <- table(predict_rfmodelNT,data_test2$winequal1)
tablerfcNT
recallrfcNT <- tablerfcNT[1,1]/(tablerfcNT[1,1]+tablerfcNT[2,1])
precisionrfcNT <- tablerfcNT[1,1]/(tablerfcNT[1,1]+tablerfcNT[1,2])
f1rfcNT <- 2/((1/precisionrfcNT)+(1/recallrfcNT))
precisionrfcNT
recallrfcNT
f1rfcNT
misclassificationrfNT <- (tablerfcNT[1,2]+tablerfcNT[2,1])/(tablerfcNT[1,1]+tablerfcNT[1,2]+tablerfcNT[2,1]+tablerfcNT[2,2])
misclassificationrfNT
gettablerfNT <- as.data.frame(resultsrfmodelNT$table)
gettablerfNT
true_goodrfNT <- gettablerfNT[1,3]
false_goodrfNT <- gettablerfNT[3,3]
false_poorrfNT <- gettablerfNT[2,3]
true_poorrfNT <- gettablerfNT[4,3]

Kappa_results <- add_row(Kappa_results, method = "random forest-not tuned", Kappa= KapparfNT,
                         Recall=recallrfcNT, Precision = precisionrfcNT, F1 = f1rfcNT, misclassification = misclassificationrfNT,
                         true_good=true_goodrfNT, false_good=false_goodrfNT, false_poor=false_poorrfNT, true_poor=true_poorrfNT)

#####
####


#Build custom AUC function to extract AUC
# from the caret model object
###this is a function created to put the data in (to create various models adjusting for bias, later)
test_roc <- function(model, data) {
  roc(data$winequal1,predict(model, data, type = "prob")[, "good"])
}

random_forest_aucNT <- rf_wineNT %>%test_roc(data = data_test2) %>% auc()
#random_forest_aucNT

```
  
  
  
### **The gradient boosted algorithm**
  
Gradient boosted algorithms^[http://uc-r.github.io/gbm_regression]  are a decision tree algorithm.  Unlike the random forest algorithm which builds an ensemble of deep decision trees, the gradient boosted model algorithm (gbm) takes an ensemble of shallow and weak successive trees with each tree learning and improving on the previous tree.  Each iteration works toward a better understanding of the unexplained variance of a previous tree.  In this way, gradient boosted algorithms are sometimes more powerful than a random forest model.  The gradient boosted algorithm performs well with weak models as it is permitted to learn slowly from past iterations.  In this report, the gradient boosted algorithm is a cross validated model which uses the caret package.  The model was trained preselecting the Bernoulli distribution as the probability model as the selection between a good and a poor wine is binomial.   The r programming for the gradient boosted model was adapted from a paper by Martin (2016)^[https://dpmartin42.github.io/posts/r/imbalanced-classes-part-1]  and Martin(2017)^[http://dpmartin42.github.io/posts/r/imbalanced-classes-part-2].    In the 2016 paper, Martin describes efforts to correct for class imbalance in the models and describes the r programming required to use sample weights to deal with the challenges of dealing with an outcome where one class heavily outweighs another. Four gradient boosted models were created (lengthy descriptions of each adjustment are included in Martin(2016) and other resources (a Silicon Valley Data Science blog post^[https://www.svds.com/learning-imbalanced-classes/]  by T. Fawcett)):  1) a gradient boosted model with no adjustment for class bias;  2) a gradient boosted model using an ‘up’ adjustment to the minority class to have the minority (good wine) class on equal count to the majority (poor wine) class; 3) a ‘down’ adjustment to the majority class to have the majority class count equal to the minority class count ;  4) a gradient boosted model using adjusted weights to impose a heavier penalty when errors are made in the minority class;  in the weighted sample, a heavier weight on a class disposes the model to classify an entity to the more heavily weighted class.
  
The original (no adjustment for class imbalance) gradient boosted model is as follows:
  
```{r gbm}

set.seed(5627)


ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)



orig_fit <- train(winequal1 ~ .,
                  data = data_train2,
                  method = "gbm",
                  distribution="bernoulli", ##for binomial##
                  verbose = FALSE,
                  metric = "ROC",
                  trControl = ctrl)



```
  
```{r gradientboostedmodels, echo=FALSE, include=FALSE}

print(orig_fit)
summary(orig_fit)
predictorigin <- predict(orig_fit, newdata=data_test2)
predictorigin
tableorigin <- table(predictorigin, data_test2$winequal1)
confusionMatrix(tableorigin)
Variable_importance_models <- add_row(Variable_importance_models,method = "gbm-original: relative influence", sulphates=17.96, alcohol=29.22, volatile.acidity=12.84,
                                      fixed.acidity=6.50, density=5.23, total.sulfur.dioxide=8.67, residual.sugar=3.41,
                                      chlorides=3.67, pH=3.14, citric.acid=6.00, free.sulfur.dioxide=3.45)

# Build custom AUC function to extract AUC
# from the caret model object

###this is a function created to put the data in (to create various models adjusting for bias, later)
test_roc <- function(model, data) {
  
  roc(data$winequal1,
      predict(model, data, type = "prob")[, "good"])
  
}




#####now, bring in the weights, to see if 'sample up' or 'sample down' would improve the results
# Create model weights (they sum to one)

model_weights <- ifelse(data_train2$winequal1 == "poor", (1/table(data_train2$winequal1)[2])*.5, (1/table(data_train2$winequal1)[1])*.5)

prop.table(table(model_weights))

ctrl$seeds <- orig_fit$control$seeds

# Build weighted model...

weighted_fit <- train(winequal1 ~ .,
                      data = data_train2,
                      method = "gbm",
                      distribution="bernoulli", ##for binomial##
                      verbose = FALSE,
                      weights = model_weights,
                      metric = "ROC",
                      trControl = ctrl)

summary(weighted_fit)
Variable_importance_models <- add_row(Variable_importance_models,method = "gbm-weighted: relative influence", sulphates=26.15, alcohol=47.88, volatile.acidity=15.81,
                                      fixed.acidity=2.92, density=0.7, total.sulfur.dioxide=3.61, residual.sugar=0.00,
                                      chlorides=1.12, pH=0.00, citric.acid=1.82, free.sulfur.dioxide=0.00)
# Build down-sampled model

ctrl$sampling <- "down"

down_fit <- train(winequal1 ~ .,
                  data = data_train2,
                  method = "gbm",
                  distribution="bernoulli", ##for binomial##
                  verbose = FALSE,
                  metric = "ROC",
                  trControl = ctrl)

summary(down_fit)
Variable_importance_models <- add_row(Variable_importance_models,method = "gbm-down: relative influence", sulphates=13.44, alcohol=43.40, volatile.acidity=11.30,
                                      fixed.acidity=3.25, density=0.80, total.sulfur.dioxide=6.20, residual.sugar=1.17,
                                      chlorides=9.02, pH=1.8, citric.acid=6.88, free.sulfur.dioxide=2.71)

# Build up-sampled model

ctrl$sampling <- "up"

up_fit <- train(winequal1 ~ .,
                data = data_train2,
                method = "gbm",
                distribution="bernoulli", ##for binomial##
                verbose = FALSE,
                metric = "ROC",
                trControl = ctrl)

summary(up_fit)
Variable_importance_models <- add_row(Variable_importance_models,method = "gbm-up: relative influence", sulphates=23.50, alcohol=47.8, volatile.acidity=13.49,
                                      fixed.acidity=2.08, density=1.38, total.sulfur.dioxide=3.86, residual.sugar=0.78,
                                      chlorides=1.98, pH=0.73, citric.acid=3.66, free.sulfur.dioxide=0.74)
Variable_importance_models <- as.data.frame(Variable_importance_models) %>% format(digits=0)



model_list <- list(original = orig_fit,
                   down = down_fit,
                   up = up_fit,
                   weighted=weighted_fit)

model_list_roc <- model_list %>%
  map(test_roc, data = data_test2)


model_list_roc %>%
  map(auc)


results_list_roc <- list(NA)
num_mod <- 1

for(the_roc in model_list_roc){
  
  results_list_roc[[num_mod]] <- 
    data_frame(tpr = the_roc$sensitivities,
               fpr = 1 - the_roc$specificities,
               model = names(model_list)[num_mod])
  
  num_mod <- num_mod + 1
  
}

results_df_roc <- bind_rows(results_list_roc)


# Plot ROC curve for all 3 models

custom_col <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00")

fourgbm <-ggplot(aes(x = fpr,  y = tpr, group = model), data = results_df_roc) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = custom_col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_bw(base_size = 18)
#fourgbm


```

|
```{r compareROC, echo=FALSE, include=FALSE}


model_list <- list(original = orig_fit,
                   weighted=weighted_fit,
                   rf=rf_wine,
                   rfNT=rf_wineNT,
                   logistic=logistic_model_cv)



model_list_roc <- model_list %>%
  map(test_roc, data = data_test2)


model_list_roc %>%
  map(auc)


results_list_roc <- list(NA)
num_mod <- 1

for(the_roc in model_list_roc){
  
  results_list_roc[[num_mod]] <- 
    data_frame(tpr = the_roc$sensitivities,
               fpr = 1 - the_roc$specificities,
               model = names(model_list)[num_mod])
  
  num_mod <- num_mod + 1
  
}

results_df_roc <- bind_rows(results_list_roc)


# Plot ROC curve for  the models

custom_col <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00") 

comparefinalmodel <- ggplot(aes(x = fpr,  y = tpr, group = model), data = results_df_roc) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = custom_col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_bw(base_size = 18)

comparefinalmodel

#get confusion matrix
###necessary to run predict function outside of the function for ROC curve


predictwted <- predict(weighted_fit, newdata=data_test2)
predictup  <- predict(up_fit, newdata=data_test2)
predictdown <- predict(down_fit, newdata=data_test2)

tableorigin <- table(predictorigin, data_test2$winequal1)
tableorigin
###get precision from the tables as predictup[1,1]/predictup[1,1]+predictup[2,1]
###get recall from the tables as predictup[1,1]/predictup[1,1]+predictup[1,2]
recallorigin <- tableorigin[1,1]/(tableorigin[1,1]+tableorigin[2,1])
precisionorigin <- tableorigin[1,1]/(tableorigin[1,1]+tableorigin[1,2])
f1origin <- 2/((1/precisionorigin)+(1/recallorigin))
precisionorigin
recallorigin
f1origin


tabledown <- table(predictdown, data_test2$winequal1)
tabledown
recalldown <- tabledown[1,1]/(tabledown[1,1]+tabledown[2,1])
precisiondown <- tabledown[1,1]/(tabledown[1,1]+tabledown[1,2])
f1down <- 2/((1/precisiondown)+(1/recalldown))
precisiondown
recalldown
f1down


###estimate precision and recalland f1 measure from tables
###precision = tP/tp+fp; recall=tp/tp+fn; f1=2/((1/precision)+(1/recall))

###get precision from the tables as predictup[1,1]/predictup[1,1]+predictup[2,1]
###get recall from the tables as predictup[1,1]/predictup[1,1]+predictup[1,2]
recallorigin <- tableorigin[1,1]/(tableorigin[1,1]+tableorigin[2,1])
precisionorigin <- tableorigin[1,1]/(tableorigin[1,1]+tableorigin[1,2])
f1origin <- 2/((1/precisionorigin)+(1/recallorigin))
precisionorigin
recallorigin
f1origin

confusionorigin <- confusionMatrix(tableorigin)
confusionorigin
getKappa <- as.data.frame(confusionorigin$overall)
Kappaorigin <- getKappa[2,]
Kappaorigin


gettableorigin <- as.data.frame(confusionorigin$table)
gettableorigin
true_goodorigin <- gettableorigin[1,3]
false_goodorigin <- gettableorigin[3,3]
false_poororigin <- gettableorigin[2,3]
true_poororigin <- gettableorigin[4,3]
misclassificationorigin <- (tableorigin[1,2]+tableorigin[2,1])/(tableorigin[1,1]+tableorigin[1,2]+tableorigin[2,1]+tableorigin[2,2])
misclassificationorigin
Kappa_results <- add_row(Kappa_results, method = "gbm_original", Kappa= Kappaorigin,
                         Recall=recallorigin, Precision = precisionorigin, F1 = f1origin, misclassification=misclassificationorigin, 
                         true_good=true_goodorigin, false_good=false_goodorigin,
                         false_poor=false_poororigin, true_poor=true_poororigin)

confusionwted <- confusionMatrix((table(predictwted,data_test2$winequal1))) 
tableweighted <-table(predictwted,data_test2$winequal1)
tableweighted
recallwtd <- tableweighted[1,1]/(tableweighted[1,1]+tableweighted[2,1])
precisionwtd <- tableweighted[1,1]/(tableweighted[1,1]+tableweighted[1,2])
f1wtd <- 2/((1/precisionwtd)+(1/recallwtd))
precisionwtd
recallwtd
f1wtd

getKappa <- as.data.frame(confusionwted$overall)
Kappawted <- getKappa[2,]
Kappawted

gettablewted <- as.data.frame(confusionwted$table)
gettablewted
true_goodwted <- gettablewted[1,3]
false_goodwted <- gettablewted[3,3]
false_poorwted <- gettablewted[2,3]
true_poorwted <- gettablewted[4,3]
misclassificationwtd <- (tableweighted[1,2]+tableweighted[2,1])/(tableweighted[1,1]+tableweighted[1,2]+tableweighted[2,1]+tableweighted[2,2])
misclassificationwtd
Kappa_results <- add_row(Kappa_results, method = "gbm_weighted", Kappa= Kappawted,
                         Recall=recallwtd, Precision = precisionwtd, F1 = f1wtd,misclassification=misclassificationwtd, 
                         true_good=true_goodwted, false_good=false_goodwted,
                         false_poor=false_poorwted, true_poor=true_poorwted)


confusionup <- confusionMatrix((table(predictup,data_test2$winequal1)))
confusionup
getKappa <- as.data.frame(confusionup$overall)
Kappaup <- getKappa[2,]
Kappaup

tableup <- table(predictup,data_test2$winequal1)
tableup
recallup <- tableup[1,1]/(tableup[1,1]+tableup[2,1])
precisionup <- tableup[1,1]/(tableup[1,1]+tableup[1,2])
f1up <- 2/((1/precisionup)+(1/recallup))
precisionup
recallup
f1up

gettableup <- as.data.frame(confusionup$table)
gettableup
true_goodup <- gettableup[1,3]
false_goodup <- gettableup[3,3]
false_poorup <- gettableup[2,3]
true_poorup <- gettableup[4,3]
misclassificationup <- (tableup[1,2]+tableup[2,1])/(tableup[1,1]+tableup[1,2]+tableup[2,1]+tableup[2,2])
misclassificationup
Kappa_results <- add_row(Kappa_results, method = "gbm_up", Kappa= Kappaup,
                         Recall=recallup, Precision = precisionup, F1 = f1up, misclassification=misclassificationup, 
                         true_good=true_goodup, false_good=false_goodup,
                         false_poor=false_poorup, true_poor=true_poorup)




confusiondown <- confusionMatrix((table(predictdown, data_test2$winequal1)))
confusiondown
getKappa <- as.data.frame(confusiondown$overall)
Kappadown <- getKappa[2,]
Kappadown

tabledown <- table(predictdown, data_test2$winequal1)
tabledown
recalldown <- tabledown[1,1]/(tabledown[1,1]+tabledown[2,1])
precisiondown <- tabledown[1,1]/(tabledown[1,1]+tabledown[1,2])
f1down <- 2/((1/precisiondown)+(1/recalldown))
precisiondown
recalldown
f1down
misclassificationdown <- (tabledown[1,2]+tabledown[2,1])/(tabledown[1,1]+tabledown[1,2]+tabledown[2,1]+tabledown[2,2])
misclassificationdown
gettabledown <- as.data.frame(confusiondown$table)
gettabledown
true_gooddown <- gettabledown[1,3]
false_gooddown <- gettabledown[3,3]
false_poordown <- gettabledown[2,3]
true_poordown <- gettabledown[4,3]

Kappa_results <- add_row(Kappa_results, method = "gbm_down", Kappa= Kappadown,
                         Recall=recalldown, Precision = precisiondown, F1 = f1down, misclassification=misclassificationdown,
                         true_good=true_gooddown, false_good=false_gooddown,
                         false_poor=false_poordown, true_poor=true_poordown)

Table_results <- as.data.frame(Kappa_results)
Table_results<- format(Table_results, digits=2)


```

### **Evaluation metrics**

As the models in this exercise are classification models, it has been determined after reading quite a few articles that the best evaluation metrics to evaluate the performance of my models are the recall and precision measures.  Recall is the number of correct positive predictions made out of all positive predictions that could have been made (all the positive predictions that could have been made is True Positives plus False Negatives).  Precision is the number of positive class predictions that actually belong to the positive class (True Positives/(True Positives plus False Positives)).  A false negative is when a success (good wine) is labelled as a failure (poor wine). A false positive is when a failure (poor wine) is labelled as a success (good wine). The F1 score is the harmonic mean of precision and recall;  for the F1 score to be high, both precision and recall need to be high. 

Martin(2016) argues that many evaluation metrics for classifiers exist, and can generally be divided into two main groups (quoting directly from the paper):
  
> Threshold-dependent: This includes metrics like accuracy, precision, recall, and F1    score, which all require a confusion matrix to be calculated using a hard cutoff on     predicted probabilities. These metrics are typically quite poor in the case of  imbalanced classes, as statistical software inappropriately uses a default threshold of 0.50 resulting in the model predicting that all observations belong in the majority class.
  
> Threshold-invariant: This includes metrics like area under the ROC curve (AUC), which quantifies true positive rate as a function of false positive rate for a variety of classification thresholds. Another way to interpret this metric is the probability that a random positive instance will have a higher estimated probability than a random negative instance.
  
  
The measures of recall, precision and the F1 score are easily produced by the R program and are fairly straightforward to understand.  A measure of the area under the ROC curve can be produced but must be viewed with other metrics.  The Kappa^[https://en.wikipedia.org/wiki/Cohen%27s_kappa] coefficient is sometimes a useful indicator, when viewed along with other indicators.  Its value is interpreted from ‘rule of thumb’, as described in McHugh(2012)^[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/].   The kappa measure is an indicator of how much the classifications of the model versus the true classification of the wine variable could be attributed to chance.    I also viewed the estimates of the contribution of the independent variables to the outcome of the model, looking for some coherence in the choice of independent variables to predict good wine.  
  
Thanks to the programming models in Martin(2016) I was able to create ROC curves (y axis is true positive rate , x axis is false positive rate) which compares the performance of the different classification models on a common grid.  The best models for identifying the ‘good’ wines will be in the regions of the plot which show the greatest area under the curve for the smallest false positive rate value (at the false positive rate between 0% and 25%).  That is, per Martin(2016),  the most successful algorithm better identifies the true positives as a function of false positives for instances that are predicted as having a high probability of being in the minority class.  In my analyses, I found that examining the ROC curve in this manner corroborated somewhat with the results of the model as represented in the output confusion matrices and the estimates of recall and precision or other performance measures.

## **Results**

### **Descriptions of Model Results**

The results of the models are presented in Table 5, below (for reference, in the test sample, the total count of good wine is 39; the total count of poor wine is 233).  The ROC curves, which I created to rate a selection of the models based upon the area of the curve between 0 – 25% on the false positive rate axis is presented immediately after.  I will refer to these when discussing the results from all models.

```{r resultstable, echo=FALSE, fig.align = 'center' ,fig.dim=c(7,5) }
kable(Table_results, caption="Evaluation metrics of all models", align='lrrrrrrrrr') %>% kable_styling(full_width=FALSE, latex_options="scale_down", font_size = 11)
```



```{r roccurves, echo=FALSE, fig.align = 'center'}

comparefinalmodel


```

It is difficult to identify which model performed ‘best’ when looking at the table.  I dive into the discussion with a comparison between the two logistic models.

The logistic model performed decently.   Attempting to adjust the train set to take into account class imbalance did improve the performance of the logistic model as the AIC dropped from 607.11 in the unadjusted train set model to an AIC of 275.07 in the ‘down’ sampled train set model.  However, taking only the AIC as an indicator of the ability of the down sampled model to predict good wine may be in error.   At this point, tests of multicollinearity have indicated that multicollinearity is not a problem in the dataset. Looking at a comparison of the two logistic curves, the AUC of the ROC curve below reveals a) the unadjusted train set model (Group 1 in the diagram) has a larger AUC  at 0.87 versus the AUC of the down sampled model at 0.84 (Group 2 in the diagram).  The down sampled model also performed more poorly (less AUC) in the 0 to 25% region of the curve (possibly indicating a lesser ability to correctly identify the minority class/good wine – although the down sampled logistic model identified 30 of 39 good wines, it also incorrectly identified 62 poor wines as ‘good’).  In the comparison between the two logistic models, the indicators for Recall and Precision are supported by the visual check on the area under the ROC in the area between 0 and 25% of fpr;  larger AUC in this region supports the notion that model is more likely to find 'good' wines and fewer false 'good' wines than the curve with less AUC in this region; larger AUC also implies a better true positive rate (better odds of correctly identifying a good wine).

What I find interesting about the ROC curves for the two logistic curve was how easy it was to produce the diagram using the MLeval package.  But if I  compare the output of the MLeval package with what was produced using ggplot and the programming to create the curves (as described in Martin(2016)), I think it is evident that the MLeval package is 'smoothing' the curves a bit.  Which is ok for a quick and dirty visual.  But, if i use these curves to identify a better performing model (looking at the bottom percent of the fpr), I should bear in mind that smoothing the rough places might have me mislead about the performance of my models and the data set underlying the curve.

```{r rocdiagramforlogistic, echo=FALSE}
comparelogisticmodels$roc

```

Moving on to look at the other models and the other performance indicators:  If we look at the Kappa coefficient, we say that the cross validated logistic model, the two random forest models have weak performance, as the values of the Kappa are 0.40 and 0.59 indicating that 15 – 30% of the data are reliably classified.  For the gradient boosted models, the Kappa values are around 0.39 or under (but over 0.2), indicating minimal performance with 4 -15% of the data reliably classified.  If we look only at the Kappa statistic, we do not see that any of the models have performed well at identifying good wine.

Next we have measures of Recall and Precision and the F1 measure. Precision answers the question: What proportion of positive identifications was actually correct? How many of the wines identified as ‘good’ are truly ‘good’. Recall answers the question: What proportion of actual positives was identified correctly? How many of the total count of good wines were identified as good.  A good model will have both recall and precision values as high as possible towards 1, to obtain an F1 measure as high as possible towards 1.  If we look only at the F1 measure, we discover, to a bit of surprise, that the cross validated logistic model (with no adjustment for class imbalance in the train set), performed second best and the ‘tuned’ random forest model performed the best of all models! (Imagine my surprise upon discovering that I am so very good at tuning a random forest model).   

If wine sellers want to be most sure, based upon a chemical analysis of wine (using the criteria described in this report), the random forest model not tuned seems the best for identifying ‘good’ wines, with only one poor wine misclassified as good (the trade off is that only eight wines are correctly classified as good, with the remainder (31) classified to poor).  And the random forest model not tuned is the best at identifying the poor wines correctly, with 232/233 correctly identified. Precision for this model is very high at 0.888.  However, recall is very low at 0.205;  a very low portion of total good wines were identified as good wine (a larger portion of total good wines were classified to poor in this model).  

Opposite to the performance of the random forest not tuned model is gbm_down (a gradient boosted model with an adjustment for class imbalance). The recall is very powerful  but the precision is very low for this model.  The gbm_down model correctly identifies 32 of 39 good wines.  Unfortunately, it also misclassifies 67 of the poor wines as ‘good’, so that less than half of wines identified as good are truly good.

The gradient boosted models were set up with very little tuning, as I have limited experience or understanding of these models as well.  I had expected them to perform better than the logistic model and the random forest model.  The gradient boosted models were very responsive to the efforts to correct for class imbalance.  More expert tuning of these models might yield better results for classifying the wines in this database.  


The ROC curves for models in the diagram are all showing a bit of discontinuity in the lower fpr values, which could mean that the train set and test set sizes are too small to create a smooth function.   In the diagram, the orange curve describes the gbm-original (no adjustment for class imbalance).  Looking at the area under the curve for false positive rate less than 0.25, the gbm_origin model shows up as the best for identifying good wines. The tuned random forest model shows the greater AUC at the 0-5% place on the x-axis.  But these observations are taken not all that seriously here:  the table above, has already indicated that none of the models in this project perform well (or, if I were to put it more positively:  each of the models has its strengths…it would be up to the user to place value on the various outputs, depending on preferences to identifying a good wine with certainty versus identifying a poor wine with certainty).

### **Variables of importance**

We can look at the Variables of Importance/relative influence of each variable in the models (Table 6), acknowledging that such a comparison between models is a bit sketchy.  Does each model use the same criteria for ranking importance?  No.  The logistic model is best analyzed for importance of contribution using the dominanceanalysis package^[https://cran.r-project.org/web/packages/dominanceanalysis/vignettes/da-logistic-regression.html] - the ranking I have listed is pretty rough.  For random forest models, the variable of importance is based upon the Accuracy measure^[https://www.displayr.com/how-is-variable-importance-calculated-for-a-random-forest/].  For the gbm, the output includes the relative influence of each variable in training the model^[https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab].  Nevertheless, I have gone ahead and put all the measures of 'importance' into one table.  What is discovered here is that there is no clearcut identification of ‘redundancy’ when comparing such a list across all model forms.


We can see that in all model forms, sulphates, alcohol and volatile.acidity are the top three in ‘importance’ to the modelling processes.  But out of 11 explanatory variables, eight are inconsistent in importance, depending on the form of the model.  This inconsistency is important:  I cannot clearly identify which of the eight lesser variables could be dropped to reduce redundancy in the model.  All variables contribute in some models.  But, for example, in three model forms, residual.sugar is not a contributor, with zero importance. But in the cross validated model, residual.sugar ranks 6th in importance.  And the table above is in conflict with the principal component analysis which identified fixed.acidity, citric.acid, pH and density as the largest contributors of the first dimension  explaining the variance in the quality variable.  The inability to clearly identify rank or importance of explanatory variables in the models is a problem to be solved by the chemist who creates the database, not the data scientist modelling data on mathematical principals.  A data scientist cannot decide why a variable should be critical to the quality of a wine.  If an important explanatory chemical variable does not show up as important in the model, then the chemist has made an error and  therefore the predictive model is in error.  Without consultation with the chemist, the data scientist cannot evaluate the model for accuracy in predicting a good wine.

```{r variables, echo=FALSE, fig.dim=c(7,5)}

kable(Variable_importance_models, caption="Variables of importance (roughly defined), all models", align='lrrrrrrrrrrr') %>% kable_styling(full_width=FALSE, latex_options="scale_down", font_size = 8)


```


### **Adjusting the threshold for predicting a good wine**

Reviewing ways to improve classification models, I learned that moving the threshold for identifying a good wine might be the best way to improve the performance of the model^[https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification] ^[https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall].

But for the wine database, another look at the distribution of wine quality across principal component dimension 1 and dimension 2 shows no indication that including the quality = 6 group in the ‘good’ wines would improve the ability of the existing models to identify good or poor wines.  The quality = 6 group is spread far and wide across the principal component dimesion grid: the inability to find a defined pattern in the quality of the wine might be due to the database itself.  The quality variable could be poorly defined.  Per the information I have, the quality was assigned by taste test.  Taste tests could be faulty and inconsistent with chemical composition of the wine. And then, as discussed above, the chemical constituents of a good wine must be clearly defined and identified otherwise the database is incoherent and modelling chemical composition using mathematical instruments will not ever identify good wine.

```{r winequalitydistribution, echo=FALSE, fig.align = 'center'}
tendency2 <- fviz_pca_ind(prcomp(clustdf), title = "PCA - wine data", 
                          habillage = winequalitystandardized$quality,  palette = "jco",
                          geom = "point", ggtheme = theme_classic(),
                          legend = "bottom")
tendency2


```

# **Conclusions**

The models tested the ability of several classification tools (logistic regression, random forest, gradient boosted algorithm) to correctly identify and classify good and poor wines.  The database for wine classification was down loaded from the Kaggle site.  The results of modelling the quality of wine using the chemical components of the wine was a very instructive exercise, even as it was discovered that the models finally produced and running would not identify good wines in any outstanding manner.

The random forest models in this project were confounded by tuning parameters.  The two random forest models came up with different results, based only upon decisions for nodes, maxnodes or number of trees selected for the model.  Mtry was offered a grid to select for the best model in each case. The random forest model seemed like a nice option able to handle a difficult data set, but as it turned out, there are many aspects to modelling with random forest that need to be understood if the model is to have reliable interpretation.  If I understood better how to set up a random forest model, I would have better faith in the results. I would prefer to see the outcome of the model defined by the dataset itself, rather than by how much guessing I could do to the depth of trees or the number of trees.   

As I have not any experience coding classification models I had quite a bit to learn.  Correlation analysis and principal component analysis produced interesting interpretations of the structures in the dataset.  But moving forward to modelling, I discovered that I could not find enough coherence between how a model behaved and what I thought I understood (from correlation analysis and principal component diagrams) about the importance of each chemical component and how these components relate to wine quality.  Without an understanding of the chemical components in the database, I cannot possibly decide which should be eliminated or more heavily weighted so that quality wine identification runs more smoothly.  Alcohol, sulphates and fixed.acidity were important leading contributors to every model but were not identified as major contributors in principal component analysis;  other components in the database had varying importance, depending upon the mathematical structure of the model. 

What I also learned from this project:  the effects of a change in the "seed" on the output of the models.  I meticulously programmed to avoid a change in the "seed", particularly in the cross validated models.  A change in the "seed" can, in some cases, move the performance of the model enough to conclude that a model is not all that reliable.  The performance measures were roughly the same, but, in the case of the cross validated logistic curves, the change was enough to reverse the AUC measures.  The results of that 'mistake' were not documented as time no longer permits more rabbit holes of investigation.

Martin(2016)^[http://dpmartin42.github.io/posts/r/imbalanced-classes-part-2] has suggested that rather than using the area under the ROC curve to identify a good model, that the area under the precision recall curve should be used.  Future projects should consider the construction of the precision recall curve.  


I did not use a ‘validation’ dataset in this project.  It did not seem to be necessary.  And, I was concerned that if I could not produce a good classification model after splitting between test and train sets, further reducing the size of the train and test set to create a validation set did not seem to be all that worthy.  I am hoping I am not heavily penalized for having no validation set.

